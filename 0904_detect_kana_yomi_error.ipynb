{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfwqQRt4BOohQv7/bwvBfN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 事前準備\n",
        "## 必要なパッケージ等を読み込み/割り当て"
      ],
      "metadata": {
        "id": "LIP0DVX3SAYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mecab-python3 unidic-lite\n",
        "! pip install plyvel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oig2C9aqrY7B",
        "outputId": "0366f3b5-73dc-4c84-d06c-eb08443f3b74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 163 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=341bced5998796bfdba956d5012a46b292acd9f5abd0c6e82da9dfa5cc203179\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite, mecab-python3\n",
            "Successfully installed mecab-python3-1.0.5 unidic-lite-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plyvel\n",
            "  Downloading plyvel-1.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (693 kB)\n",
            "\u001b[K     |████████████████████████████████| 693 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: plyvel\n",
            "Successfully installed plyvel-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests as reqs\n",
        "import json\n",
        "import regex\n",
        "import plyvel\n",
        "import pickle\n",
        "import MeCab"
      ],
      "metadata": {
        "id": "l_CkY7fnR_5g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 作業ディレクトリ（ファイルの読み込みや保存を行う絶対パス）"
      ],
      "metadata": {
        "id": "CpKIv31V__92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "APP_WORKDIR = \"/content/drive/MyDrive/coefont_kana_converter_error_detector/\""
      ],
      "metadata": {
        "id": "Qsg_nah3_ipM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omnti7xwR5FQ",
        "outputId": "d19e95ea-3492-452e-b3fd-299b401b26a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### APIのアクセストークンなど機密情報の管理"
      ],
      "metadata": {
        "id": "eSU3i3VYR5cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "secrets = {}\n",
        "with open(APP_WORKDIR + 'secrets.json') as f:\n",
        "  secrets = json.load(f)"
      ],
      "metadata": {
        "id": "cA9ux2ZOR9pE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データベースのセットアップ（品詞の保存）\n",
        "* APIのアクセス量を減らすため、過去に変換を行った単語をデータベースに保存し、キャッシュとして利用する。\n",
        "* 読み書きの早い、Key -> Value型のローカルファイル型データベースである。\n",
        "* Ethereumのノードの内部ではトランザクションの管理とかでも使われている。\n",
        "* https://github.com/google/leveldb\n",
        "* https://plyvel.readthedocs.io/en/latest/"
      ],
      "metadata": {
        "id": "Enp-uLRI-sCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_db = plyvel.DB(APP_WORKDIR + 'vocabs.ldb', create_if_missing=True)\n",
        "lebels_db = plyvel.DB(APP_WORKDIR + 'lebels.ldb', create_if_missing=True)\n",
        "checked_db = plyvel.DB(APP_WORKDIR + 'checked.ldb', create_if_missing=True)\n",
        "\n",
        "dbs = [vocab_db, lebels_db, checked_db]"
      ],
      "metadata": {
        "id": "hG52BVjO-q4A"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### パスを間違えたりして再読み込みする場合は以下セルを実行してから上のセルを実行する"
      ],
      "metadata": {
        "id": "wEDPI9QY_zsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for db in dbs:\n",
        "  db.close()"
      ],
      "metadata": {
        "id": "z_bvONVi_wGX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各種依存記号の設定"
      ],
      "metadata": {
        "id": "qEWp89VHTI3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYMBOL_READING_POINT = \"、\" # 句読点\n",
        "SYMBOL_PUNCTUATION = \"。\" # 読点\n",
        "SYMBOL_HALF_SPACE = \" \" # 半角スペース\n",
        "SYMBOL_LONG_NOTE = \"ー\" # 伸ばし棒\n",
        "SYMBOL_NONE = \"\" # 空文字\n",
        "SYMBOL_TAB = \"\\t\" # TAB\n",
        "SYMBOL_SEMICORON = \";\" # SEMICORON"
      ],
      "metadata": {
        "id": "LAPCsUi5TINt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データソース別のクラス\n",
        "* 単語中抽出に必要な対象（Webページなど）を管理する\n",
        "* 名詞の抽出とMeCabによるよみラベルを管理する。\n",
        "\n",
        "## 要件\n",
        "*   Webサイトからテキストをスクレイピング（Webサイトごとにテンプレートを作成する）\n",
        "*   リンクや特殊記号を排除する\n",
        "*   文章の形態素解析を行い、漢字が含まれている名詞のみを抽出する。\n",
        "\n"
      ],
      "metadata": {
        "id": "VlMjUzYgOt88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MecabのWakatiで初期化"
      ],
      "metadata": {
        "id": "hsYGGJU30m8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wakati = MeCab.Tagger()"
      ],
      "metadata": {
        "id": "CSDVVldM0jXR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 漢字判定用の正規表現"
      ],
      "metadata": {
        "id": "VNtVNOTQ0j2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KANJI_REG_PETERN = regex.compile(r'\\p{Script=Han}+')"
      ],
      "metadata": {
        "id": "Ym4Waerl0Dwy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ローダー"
      ],
      "metadata": {
        "id": "uRtAIwNx2E4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OTxjS4vLOpg_"
      },
      "outputs": [],
      "source": [
        "class DataSource:\n",
        "  def __init__(self):\n",
        "    self.vocabs = []\n",
        "    self.labels = [] \n",
        "\n",
        "  def _fetchData(self): # 文章（テキストデータ）を読み込む、ここは各サービスごとに取得の方法が異なるためオーバーライドする。\n",
        "    pass\n",
        "\n",
        "  def preprosessing(self, subject):\n",
        "    handled = self._replaceSymbol(subject) #ここから文字列\n",
        "    return handled\n",
        "\n",
        "  def _replaceSymbol(self, subject):\n",
        "    sentence = subject.replace(SYMBOL_TAB, SYMBOL_NONE)\n",
        "    return sentence\n",
        "\n",
        "  def load(self):\n",
        "    text = self._fetchData()\n",
        "    text = self.preprosessing(text)\n",
        "    vocabs, labels = self._analysis(text)\n",
        "    self.vocabs = vocabs\n",
        "    self.labels = labels \n",
        "\n",
        "  def _analysis(self, subject):\n",
        "    result = wakati.parse(subject) # 形態素解析分析 tab分けで結果が出てくる\n",
        "    vocabs = [line.split(SYMBOL_TAB) for line in result.splitlines()]\n",
        "    nouns = [vocab_data for vocab_data in vocabs if len(vocab_data) >= 4 and \"名詞\" in vocab_data[4][0:2] and KANJI_REG_PETERN.search(vocab_data[0])] # 漢字を含む名詞のみ抽出\n",
        "    nouns, labels = [noun[0] for noun in nouns],  [noun[2] for noun in nouns] # 品詞とかな変換を分割\n",
        "    unique_nouns = []\n",
        "    unique_labels = []\n",
        "    for noun, label in zip(nouns, labels):\n",
        "      if noun not in unique_nouns:\n",
        "        unique_nouns.append(noun)\n",
        "        unique_labels.append(label)\n",
        "    return unique_nouns, unique_labels # 名詞とMeCabによるかな変換を取得\n",
        "\n",
        "  def save(self, new_count=False):\n",
        "    reg_vocab_count = 0\n",
        "    for vocab, label in zip(self.vocabs, self.labels):\n",
        "      binary_key = vocab.encode(\"utf-8\") # key for level db\n",
        "      binary_lebel = label.encode(\"utf-8\") # key for level db\n",
        "      if new_count and vocab_db.get(binary_key) is None:\n",
        "        reg_vocab_count +=1\n",
        "      vocab_db.put(binary_key, binary_lebel)\n",
        "    return reg_vocab_count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各種サービスなど、オケージョンごとのローダー\n",
        "\n",
        "ローダーの対象\n",
        "*   note.com （記事サービス）\n",
        "*   wikipedia.org (辞書)\n",
        "*   ローカルファイル\n",
        "*   変数\n",
        "\n"
      ],
      "metadata": {
        "id": "6HYBpVtp2GpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 変数からテキストを読み込む"
      ],
      "metadata": {
        "id": "HeSgc6ik0_72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArrayDataSource(DataSource):\n",
        "  def __init__(self, vocabs, labels):\n",
        "    super().__init__()\n",
        "    self.vocabs = vocabs\n",
        "    self.labels = labels \n",
        "\n",
        "  def load(self):\n",
        "    pass\n",
        "\n",
        "  def _analysis(self, subject):\n",
        "    pass\n",
        "\n",
        "  def _fetchData(self):\n",
        "    return \"\".self.vocabs"
      ],
      "metadata": {
        "id": "kZur9LBQxBnt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ローカルのテキストファイルからテキストを読み込む（バイナリファイル未対応）"
      ],
      "metadata": {
        "id": "YDovBKdZ1yhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalDataSource(DataSource):\n",
        "  def __init__(self, file_path):\n",
        "    super().__init__()\n",
        "    self.file_path = file_path\n",
        "\n",
        "  def _fetchData(self):\n",
        "    with open(self.file_path) as f:\n",
        "      lines = f.readlines()\n",
        "      self.source = \"\".join(lines)\n",
        "    return self.source"
      ],
      "metadata": {
        "id": "dCKdod141LF1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note API v2\n",
        "note.comから記事を取得する"
      ],
      "metadata": {
        "id": "niCqReDO1uOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoteDataSource(DataSource):\n",
        "  def __init__(self, note_article_key):\n",
        "    super().__init__()\n",
        "    self.note_article_key = note_article_key\n",
        "\n",
        "  def _fetchData(self):\n",
        "    url = \"https://note.com/api/v1/notes/{}\".format(self.note_article_key)\n",
        "    response = reqs.get(url=url)\n",
        "    text = response.text\n",
        "    response_json = json.loads(text)\n",
        "    return response_json[\"data\"][\"body\"]"
      ],
      "metadata": {
        "id": "NBfLg5yC02Pv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia\n",
        "\n",
        "https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles=%E6%85%B6%E6%87%89%E7%BE%A9%E5%A1%BE%E5%A4%A7%E5%AD%A6"
      ],
      "metadata": {
        "id": "bz-PMmHI2apr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WikipediaDataSource(DataSource):\n",
        "  def __init__(self, title):\n",
        "    super().__init__()\n",
        "    self.title = title\n",
        "\n",
        "  def _fetchData(self):\n",
        "    url = \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1\"\n",
        "    params = {\n",
        "        \"titles\": self.title,\n",
        "    }\n",
        "    response = reqs.get(url=url, params=params)\n",
        "    text = response.text\n",
        "    response_json = json.loads(text)\n",
        "    return list(response_json[\"query\"][\"pages\"].values())[0].get(\"extract\", \"\")"
      ],
      "metadata": {
        "id": "UR8adA3m08z4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一時開発用データセット"
      ],
      "metadata": {
        "id": "RpJhw7bl62Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TempDataSource(DataSource):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  # loadを上書き\n",
        "  def load(self):\n",
        "    note_articles_id = [\n",
        "      \"n4ece27ed112b\",\n",
        "      \"n0433243163f5\",\n",
        "      \"n2861fae03861\",\n",
        "      \"n6965c22952ac\",\n",
        "      \"ndf3aed673e7f\",\n",
        "      \"n766dc842145d\",\n",
        "      \"ndace078a3cb1\"\n",
        "    ]\n",
        "    data_sources = [NoteDataSource(id) for id in note_articles_id]\n",
        "    nouns = []\n",
        "    labels = []\n",
        "    for data_source in data_sources:\n",
        "      data_source.load()\n",
        "      nouns = nouns + data_source.vocabs\n",
        "      labels = labels + data_source.labels\n",
        "    # self.vocabs = nouns[:83]\n",
        "    # self.labels = labels[:83]\n",
        "    self.vocabs = nouns\n",
        "    self.labels = labels"
      ],
      "metadata": {
        "id": "5ZJ54oqW64Tv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各種ローダーのテスト"
      ],
      "metadata": {
        "id": "dYRPRiTr18CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test01_note_datasource = NoteDataSource(note_article_key=\"n4ece27ed112b\")\n",
        "test01_note_datasource.load()\n",
        "\n",
        "print(test01_note_datasource.vocabs)\n",
        "print(test01_note_datasource.labels)\n",
        "\n",
        "test01_note_datasource.save(new_count=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbxdLpJqyQ8I",
        "outputId": "418753b9-4182-4e56-c4e5-5058945c62e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['大学', '入学', '時代', '人間', '選択', '幸せ', '伝播', '自分', '不安', '勉強', '自負', '技術', '修得', '人', '一', '倍', '熱量', '努力', '実際', '学生', '身分', '個人', '開発', '仕事', '社会', '感覚', '自身', '生活', '半ば', '先生', '方', '言葉', '魂', '所属', '研究', '会', '友人', '価値', '方向', '言語', '学び', '本質', '事実', '記事', '今後', '記録', '執筆', '迷走', '逆', '体得', '宝物', '月', '終わり', '勢い', '文中', '箇所', '筆者', '教え', '理解', '問題', '発見', '解決', '最近', '以外', '中等', '教育', '学校', '反面', '事例', '背景', '論点', '整理', '視点', '在学', '骨', '髄', '変容', '意見', '参考', '本当', '少数', '解釈', '考え', '注意', '公式', '見解', '一切', '独断', '偏見', '構成', '主張', '念頭', '進学', '意図', '特徴', '理系', '文系', '区別', '自由', '分野', '横断', '内外', '耳', '着眼', '点', '縦割り', '意識', '必要', '対応', '世界', '帰着', '複雑', '議論', '担保', '題材', '享受', '世の中', '発展', '環境', '注目', '例', '導入', '暮らし', '以前', '単位', '失敗', '典型', '話題', '目的', '不在', '情報', '恩恵', '問い', '探索', '方法', '論', '先行', '取り組み', '事', '欲望', '合理', '道筋', '分断', '限り', '人々', '単体', '寄与', '根幹', '後者', '先導', '人材', '育成', '力', '最終', '身', '年間', '結果', '創出', '伝達', '付加', '創造', '連鎖', '今', '命題', '量産', '輸出', '変化', '通り', '知識', '問いかけ', '性質', '着目', '高校', '思考', '段', '指導', '要領', '受験', '上', '補完', '大体', '偏差', '目標', '塾', '多く', '見直し', '先ほど', '未来', '現在', '逆算', '実現', '部分', '気づき', '時間', '頭', '番', '学部', '魅力', '他', '評価', '軸', '自ら', '場所', '全知', '全能', '認識', '自走', '維持', '原点', '生産', '活動', '土台', '要約', '具材', '存在', '深み', '説得', '要求', '学問', '幅', '用意', '構造', '体', '能力', '主眼', '基本', '主', '必然', '現象', '自体', '最初', '脱却', '用語', '集中', '経営', '達成', '勢力', '事業', '拡大', '話', '説明', '本格', '前', '合格', '後', '形', '経験', '皆', '裏', '相対', '革新', '意義', '巷', '機会', '二', '条件', '分類', '頭出し', '趣旨', '専攻', '期待', '空間', '浸透', '現状', '素養', '場面', '強み', '理由', '確立', '余白', '子孫', '重点', '目', '共有']\n",
            "['ダイガク', 'ニュウガク', 'ジダイ', 'ニンゲン', 'センタク', 'シアワセ', 'デンパ', 'ジブン', 'フアン', 'ベンキョウ', 'ジフ', 'ギジュツ', 'シュウトク', 'ヒト', 'イチ', 'バイ', 'ネツリョウ', 'ドリョク', 'ジッサイ', 'ガクセイ', 'ミブン', 'コジン', 'カイハツ', 'シゴト', 'シャカイ', 'カンカク', 'ジシン', 'セイカツ', 'ナカバ', 'センセイ', 'ホウ', 'コトバ', 'タマシイ', 'ショゾク', 'ケンキュウ', 'カイ', 'ユウジン', 'カチ', 'ホウコウ', 'ゲンゴ', 'マナビ', 'ホンシツ', 'ジジツ', 'キジ', 'コンゴ', 'キロク', 'シッピツ', 'メイソウ', 'ギャク', 'タイトク', 'タカラモノ', 'ガツ', 'オワリ', 'イキオイ', 'ブンチュウ', 'カショ', 'ヒッシャ', 'オシエ', 'リカイ', 'モンダイ', 'ハッケン', 'カイケツ', 'サイキン', 'イガイ', 'チュウトウ', 'キョウイク', 'ガッコウ', 'ハンメン', 'ジレイ', 'ハイケイ', 'ロンテン', 'セイリ', 'シテン', 'ザイガク', 'ホネ', 'ズイ', 'ヘンヨウ', 'イケン', 'サンコウ', 'ホントウ', 'ショウスウ', 'カイシャク', 'カンガエ', 'チュウイ', 'コウシキ', 'ケンカイ', 'イッサイ', 'ドクダン', 'ヘンケン', 'コウセイ', 'シュチョウ', 'ネントウ', 'シンガク', 'イト', 'トクチョウ', 'リケイ', 'ブンケイ', 'クベツ', 'ジユウ', 'ブンヤ', 'オウダン', 'ナイガイ', 'ミミ', 'チャクガン', 'テン', 'タテワリ', 'イシキ', 'ヒツヨウ', 'タイオウ', 'セカイ', 'キチャク', 'フクザツ', 'ギロン', 'タンポ', 'ダイザイ', 'キョウジュ', 'ヨノナカ', 'ハッテン', 'カンキョウ', 'チュウモク', 'レイ', 'ドウニュウ', 'クラシ', 'イゼン', 'タンイ', 'シッパイ', 'テンケイ', 'ワダイ', 'モクテキ', 'フザイ', 'ジョウホウ', 'オンケイ', 'トイ', 'タンサク', 'ホウホウ', 'ロン', 'センコウ', 'トリクミ', 'コト', 'ヨクボウ', 'ゴウリ', 'ミチスジ', 'ブンダン', 'カギリ', 'ヒトビト', 'タンタイ', 'キヨ', 'コンカン', 'コウシャ', 'センドウ', 'ジンザイ', 'イクセイ', 'チカラ', 'サイシュウ', 'ミ', 'ネンカン', 'ケッカ', 'ソウシュツ', 'デンタツ', 'フカ', 'ソウゾウ', 'レンサ', 'イマ', 'メイダイ', 'リョウサン', 'ユシュツ', 'ヘンカ', 'トオリ', 'チシキ', 'トイカケ', 'セイシツ', 'チャクモク', 'コウコウ', 'シコウ', 'ダン', 'シドウ', 'ヨウリョウ', 'ジュケン', 'ウエ', 'ホカン', 'ダイタイ', 'ヘンサ', 'モクヒョウ', 'ジュク', 'オオク', 'ミナオシ', 'サキホド', 'ミライ', 'ゲンザイ', 'ギャクサン', 'ジツゲン', 'ブブン', 'キヅキ', 'ジカン', 'アタマ', 'バン', 'ガクブ', 'ミリョク', 'タ', 'ヒョウカ', 'ジク', 'ミズカラ', 'バショ', 'ゼンチ', 'ゼンノウ', 'ニンシキ', 'ジソウ', 'イジ', 'ゲンテン', 'セイサン', 'カツドウ', 'ドダイ', 'ヨウヤク', 'グザイ', 'ソンザイ', 'フカミ', 'セットク', 'ヨウキュウ', 'ガクモン', 'ハバ', 'ヨウイ', 'コウゾウ', 'タイ', 'ノウリョク', 'シュガン', 'キホン', 'シュ', 'ヒツゼン', 'ゲンショウ', 'ジタイ', 'サイショ', 'ダッキャク', 'ヨウゴ', 'シュウチュウ', 'ケイエイ', 'タッセイ', 'セイリョク', 'ジギョウ', 'カクダイ', 'ハナシ', 'セツメイ', 'ホンカク', 'マエ', 'ゴウカク', 'アト', 'カタチ', 'ケイケン', 'ミナ', 'ウラ', 'ソウタイ', 'カクシン', 'イギ', 'チマタ', 'キカイ', 'ニ', 'ジョウケン', 'ブンルイ', 'アタマダシ', 'シュシ', 'センコウ', 'キタイ', 'クウカン', 'シントウ', 'ゲンジョウ', 'ソヨウ', 'バメン', 'ツヨミ', 'リユウ', 'カクリツ', 'ヨハク', 'シソン', 'ジュウテン', 'メ', 'キョウユウ']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test01_wiki_datasource = WikipediaDataSource(title=\"慶應義塾大学\")\n",
        "test01_wiki_datasource.load()\n",
        "\n",
        "print(test01_wiki_datasource.vocabs)\n",
        "print(test01_wiki_datasource.labels)\n",
        "\n",
        "test01_wiki_datasource.save(new_count=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiTVUKj-4PGY",
        "outputId": "52657faa-0788-4f94-d2aa-f52d3c2ee191"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['慶應', '義塾', '大学', '英語', '東京', '都', '港', '区', '三田', '丁目', '番', '号', '本部', '日本', '私立', '年', '創立', '設置', '略称', '慶大', '應', '旧', '字体', '報道', '慶応', '表記']\n",
            "['ケイオウ', 'ギジュク', 'ダイガク', 'エイゴ', 'トウキョウ', 'ト', 'ミナト', 'ク', 'ミタ', 'チョウメ', 'バン', 'ゴウ', 'ホンブ', 'ニッポン', 'シリツ', 'ネン', 'ソウリツ', 'セッチ', 'リャクショウ', 'ケイダイ', '應', 'キュウ', 'ジタイ', 'ホウドウ', 'ケイオウ', 'ヒョウキ']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# コンバーターのクラス\n",
        "\n",
        "* 漢字をかな変換するコンバータ\n",
        "* クラスとして機能を丸めることで複数のコンバーターの差異を吸収する。コンバーターを適用する順序を入れ替えるなど"
      ],
      "metadata": {
        "id": "fXVrUefMPnF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseConverter:\n",
        "  def __init__(self, conveter_name):\n",
        "    self.converter_name = conveter_name\n",
        "    self.split_code = SYMBOL_SEMICORON\n",
        "\n",
        "  def preprosessing(self, subject):\n",
        "    return subject\n",
        "\n",
        "  def convert(self, vocabs):\n",
        "    sentence = SYMBOL_PUNCTUATION.join(vocabs) + SYMBOL_PUNCTUATION # 各単語をsplit tokenを付けて文字列化\n",
        "    result = self._execute_api(sentence)\n",
        "    # result = result.replace(SYMBOL_PUNCTUATION, SYMBOL_NONE)\n",
        "    converted_vocabs = [self.preprosessing(vocab) for vocab in result.split(SYMBOL_PUNCTUATION)] #文字列をsplit tokenを用いて単語ごとにリスト化\n",
        "    return converted_vocabs\n",
        "\n",
        "  def get_indexkey(self, noun):\n",
        "    return \"{}_{}\".format(self.converter_name, noun).encode(\"utf-8\")"
      ],
      "metadata": {
        "id": "8NiZ62R3RxUz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoeFont API (target)\n",
        "* CoefontのAPIのアクセス方法とアクセスキーが変わり次第、実装する"
      ],
      "metadata": {
        "id": "8LoJLI8UOGZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYMBOL_TRIGGER_1 = [\n",
        "        [\"オ\", \"コ\", \"ソ\", \"ト\", \"ノ\", \"ホ\", \"モ\", \"ヨ\", \"ロ\", \"ヲ\"], # o\n",
        "        [\"エ\", \"ケ\", \"セ\", \"ネ\", \"ヘ\", \"メ\", \"レ\"], # e removed テ\n",
        "        [\"ア\", \"カ\", \"サ\", \"タ\", \"ナ\", \"ハ\", \"マ\", \"ラ\", \"ヤ\", \"ラ\", \"ワ\"], # a\n",
        "        [\"ュ\", \"ョ\"],\n",
        "        [\"ゴ\", \"ゾ\", \"ド\", \"ボ\", \"ポ\", \"ゴ\", \"ゾ\", \"ド\", \"ボ\"],\n",
        "        [\"ギ\", \"ジ\", \"ヂ\", \"ビ\", \"ピ\", \"ギ\", \"ジ\", \"ジ\", \"ビ\",\"ピ\"],\n",
        "        [\"イ\", \"キ\", \"シ\", \"千\", \"二\", \"ヒ\", \"ミ\", \"リ\"], # i \n",
        "        [\"ウ\", \"ク\", \"ス\", \"ツ\", \"ヌ\", \"フ\", \"ム\", \"ユ\", \"ル\"] # u\n",
        "        ]\n",
        "\n",
        "SYMBOL_TRIGGER_2 = [\n",
        "        [\"ウ\"],\n",
        "        [\"イ\"],\n",
        "        [\"ア\"],\n",
        "        [\"ウ\"],\n",
        "        [\"ウ\"],\n",
        "        [\"イ\"],\n",
        "        [\"イ\"],\n",
        "        [\"ウ\"]\n",
        "        ]\n",
        "\n",
        "class CoeFontConverter(BaseConverter):\n",
        "  def __init__(self):\n",
        "    super().__init__(\"coefont\")\n",
        "\n",
        "  def _execute_api(self, sentence):\n",
        "    # coefontの実装\n",
        "    url = secrets[\"coefont_api_endpoint\"]\n",
        "    params = {\n",
        "        \"text\": sentence,\n",
        "    }\n",
        "\n",
        "    r = reqs.post(url=url, data=params)\n",
        "    response_text = r.text\n",
        "    print(response_text)\n",
        "    response_json = json.loads(response_text)\n",
        "    return response_json[\"yomi\"]\n",
        "\n",
        "  def preprosessing(self, subject):\n",
        "    list_kat_subject = list(subject)\n",
        "    kat_subject_size = len(list_kat_subject)-1\n",
        "    idkc = 0\n",
        "    while  idkc < kat_subject_size:\n",
        "      kat_char_pointer = list_kat_subject[idkc]\n",
        "      kat_char_next = list_kat_subject[idkc+1]\n",
        "      if kat_char_next == SYMBOL_LONG_NOTE:\n",
        "         for ids, symbols in enumerate(SYMBOL_TRIGGER_1):\n",
        "           if kat_char_pointer in symbols:\n",
        "               list_kat_subject[idkc+1] = SYMBOL_TRIGGER_2[ids][0]\n",
        "               idkc+=1\n",
        "      idkc+=1\n",
        "    return \"\".join(list_kat_subject)"
      ],
      "metadata": {
        "id": "nHuLN8qLQOfu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 一時的開発用コンバータ\n",
        "\n",
        "*   CoeFontの出力を想定して、特定の単語を入力した静的な結果を返却する\n",
        "\n"
      ],
      "metadata": {
        "id": "7QlXXq2k8bRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TempCoeFontConverter(CoeFontConverter):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def _execute_api(self, sentence):\n",
        "    # coefontの実装\n",
        "    # 一時的な実装 決まった文章を返す\n",
        "    source = None\n",
        "    with open(APP_WORKDIR + \"response_commma.json\") as f:\n",
        "      source = json.load(f)\n",
        "    return source[\"yomi\"]"
      ],
      "metadata": {
        "id": "y5NXAUpe8Qwn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goo API\n",
        "* かな変換APIを使用する。\n",
        "* ドキュメントはこちら\n",
        "  * https://labs.goo.ne.jp/api/jp/hiragana-translation/"
      ],
      "metadata": {
        "id": "Hnp4QEbQOLne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GooConverter(BaseConverter):\n",
        "  def __init__(self, app_id):\n",
        "    super().__init__(\"goo\")\n",
        "    self.app_id = app_id\n",
        "\n",
        "  def _execute_api(self, sentence):\n",
        "    url = \"https://labs.goo.ne.jp/api/hiragana\"\n",
        "    params = {\n",
        "        \"app_id\": self.app_id,\n",
        "        \"sentence\": sentence,\n",
        "        \"output_type\": \"katakana\"\n",
        "    }\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    r = reqs.post(url=url, data=params)\n",
        "    response_text = r.text\n",
        "    print(\"** Goo API:\", response_text)\n",
        "    response_json = json.loads(response_text)\n",
        "    return response_json.get(\"converted\", \"\")\n",
        "\n",
        "  def preprosessing(self, subject):\n",
        "     return subject.replace(SYMBOL_HALF_SPACE, SYMBOL_NONE)"
      ],
      "metadata": {
        "id": "VhNvRmfDvruk"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yahoo API\n",
        "* かな変換APIを使用する。\n",
        "* ドキュメントはこちら\n",
        "  * https://developer.yahoo.co.jp/webapi/jlp/furigana/v2/furigana.html\n",
        "  * https://developer.yahoo.co.jp/webapi/jlp/sample/sample10.html"
      ],
      "metadata": {
        "id": "nBQpqPWhHcWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YahooConverter(BaseConverter):\n",
        "  def __init__(self, app_id):\n",
        "    super().__init__(\"yahoo\")\n",
        "    self.app_id = app_id\n",
        "\n",
        "  def _execute_api(self, sentence):\n",
        "    url = \"https://jlp.yahooapis.jp/FuriganaService/V2/furigana\"\n",
        "    params = {\n",
        "      \"id\": \"abc\",\n",
        "      \"jsonrpc\": \"2.0\",\n",
        "      \"method\": \"jlp.furiganaservice.furigana\",\n",
        "      \"params\": {\n",
        "        \"q\": sentence,\n",
        "      }\n",
        "    }\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"User-Agent\": \"Yahoo AppID: {}\".format(self.app_id),\n",
        "    }\n",
        "\n",
        "    params = json.dumps(params).encode()\n",
        "    r = reqs.post(url=url, data=params, headers=headers)\n",
        "    response_text = r.text\n",
        "    print(\"** Yahoo API:\", response_text)\n",
        "    response_json = json.loads(response_text)\n",
        "    kana_list = [word.get(\"furigana\", word.get(\"surface\", \"ERR\")) for word in response_json[\"result\"][\"word\"]]\n",
        "    return SYMBOL_NONE.join(kana_list)\n",
        "\n",
        "  def _hiragana_to_katakana(self, target):\n",
        "    return ''.join([chr(n+96) if (12352 < n and n < 12439) or n==12445 or n==12446 else chr(n) for n in [ord(c) for c in target]])\n",
        "\n",
        "  def preprosessing(self, subject):\n",
        "    subject = self._hiragana_to_katakana(subject)\n",
        "    subject = subject.replace(SYMBOL_HALF_SPACE, SYMBOL_NONE)\n",
        "    return subject"
      ],
      "metadata": {
        "id": "5iKa08M4Hi9h"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データソース毎に読みの検証を行う\n",
        "* コンバータを用いてCoeFontの単語出力と比較を行う。\n",
        "* 比較の結果、不一致だった場合は他のコンバーターにおいても比較を行い、正解ラベルを付与できるよう分布を収束させる。"
      ],
      "metadata": {
        "id": "8n6ZLt9ZPd09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConverterCompetition:\n",
        "  def __init__(self, converters):\n",
        "    self.converters = converters\n",
        "\n",
        "  # 指定されたデータソースをもとに、指定されたコンバータでかな変換を行う。\n",
        "  # 名詞と、かな変換の結果をタプル型で返却する。\n",
        "  def _convert_all_on(self, data_source, converter_id, reset_cache=False):\n",
        "    vocabs = data_source.vocabs\n",
        "    converter = self.converters[converter_id] # 指定されたコンバータを取得\n",
        "    db_keys = [converter.get_indexkey(vocab) for vocab in vocabs] # 名詞をデータベースに対応するIDに変換\n",
        "\n",
        "    caches = []\n",
        "    if reset_cache:\n",
        "      caches = [None for db_key in db_keys] # キャッシュ削除\n",
        "    else:\n",
        "      caches = [lebels_db.get(db_key) for db_key in db_keys] # データベースに問い合わせる。キャッシュがなかったらNoneが設定される\n",
        "\n",
        "    entred_vocab = [vocab for idv, vocab in enumerate(vocabs) if caches[idv] is None] # キャッシュが存在せず、新しくAPIから取得する名詞一覧\n",
        "    new_yomi_labels = [] # 変換された読みを保存するためのリスト\n",
        "\n",
        "    if (len(entred_vocab) > 0): # 新しくAPIからかな変換を取得する必要があるか確かめる\n",
        "      new_yomi_labels = converter.convert(entred_vocab)\n",
        "\n",
        "    for vocab, label in zip(entred_vocab, new_yomi_labels):\n",
        "      lebels_db.put(converter.get_indexkey(vocab), label.encode(\"utf-8\")) # APIから取得したよみをデータベースに保存（キャッシュとなり、以降これが使われる）\n",
        "\n",
        "    yomi_labels = []\n",
        "    iter_new_yomi_labels = iter(new_yomi_labels)\n",
        "    for idc, cache in enumerate(caches):\n",
        "      if cache:\n",
        "        yomi_labels.append(cache.decode(\"utf-8\")) # キャッシュ（db）にある場合はキャッシュから取得\n",
        "      else:\n",
        "        yomi_labels.append(next(iter_new_yomi_labels)) # ない場合は、先ほど変換した結果から取得\n",
        "    \n",
        "    return vocabs, yomi_labels\n",
        "\n",
        "  # 指定されたデータソースをもとに、指定されたコンバータ間で結果を比較する\n",
        "  # 不一致の単語とその結果をタプル型で返却する\n",
        "  def compete_to(self, data_source, subject_converter_id, target_converter_id, reset_cache=False):\n",
        "    vocabs, yomi_labels_0 = self._convert_all_on(data_source, subject_converter_id, reset_cache)\n",
        "    _, yomi_labels_1 = self._convert_all_on(data_source, target_converter_id, reset_cache)\n",
        "\n",
        "    errors = []\n",
        "    for idn, noun in enumerate(vocabs):\n",
        "      yomi_label_0 = yomi_labels_0[idn]\n",
        "      yomi_label_1 = yomi_labels_1[idn]\n",
        "      if yomi_label_0 != yomi_label_1: # 読みラベルを比較して、一致しなかった場合errorsに追加\n",
        "         errors.append([noun, yomi_label_0, yomi_label_1])\n",
        "\n",
        "    return errors\n",
        "\n",
        "  # 指定されたデータソースをもとに、Mecabと指定されたコンバータとの結果を比較する\n",
        "  # 不一致の単語とその結果をタプル型で返却する\n",
        "  def compete_to_mecab(self, data_source, converter_id=0, reset_cache=False):\n",
        "    vocabs, yomi_labels = self._convert_all_on(data_source, converter_id, reset_cache)\n",
        "\n",
        "    errors = []\n",
        "    for idn, noun in enumerate(vocabs):\n",
        "      yomi_label = yomi_labels[idn]\n",
        "      mecab_label = data_source.labels[idn]\n",
        "      if yomi_label != mecab_label: # 読みラベルを比較して、一致しなかった場合errorsに追加\n",
        "         errors.append([noun, mecab_label + \":mecab\", yomi_label+\":c_{}\".format(converter_id)]) # 単語, 比較対象の出力, mecabの出力\n",
        "\n",
        "    return errors\n",
        "\n",
        "  # 指定されたデータソースををもとに、すべてのコンバータの結果を表示する、\n",
        "  # ただし、対象はMeCabとCoeFontの比較で不一致の名詞のみ\n",
        "  # 不一致の単語に対する各コンバーター結果を辞書型で返却する。\n",
        "  def compete_to_all(self, data_source, reset_cache=False):\n",
        "    # MeCabのエラー抽出\n",
        "    errors_0 = self.compete_to_mecab(data_source, 0, reset_cache) # mecabとconverter 0を比較する\n",
        "    errors = {error_0[0]:[error_0[1], error_0[2]] for error_0 in errors_0} #エラーを格納するdict \n",
        "    errors_0_nouns = errors.keys()\n",
        "\n",
        "    # MeCabと一致しなかった単語を外部APIにかける、\n",
        "    for idc in range(1, len(self.converters)):\n",
        "      array_dataset = ArrayDataSource(errors_0_nouns, []) # 対象の単語をデータソース化\n",
        "      vocabs, yomi_labels = self._convert_all_on(array_dataset, idc, reset_cache) #APIの結果を取得\n",
        "      for checked_vocab, checked_label in zip(vocabs, yomi_labels):\n",
        "        errors[checked_vocab].append(checked_label)\n",
        "    return errors\n"
      ],
      "metadata": {
        "id": "FHm61F4SQOyl"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 誤字の検出と、複数APIを用いた結果の分布を取得する"
      ],
      "metadata": {
        "id": "MbPtolIyDVIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データソースのロード"
      ],
      "metadata": {
        "id": "jhcqurD9FxZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_04_datasource = TempDataSource()\n",
        "test_04_datasource.load() # 単語の分割とMeCabのラベルを取得する\n",
        "\n",
        "# データソースの一致を確認\n",
        "test_04_result = TempCoeFontConverter().convert([])\n",
        "print(len((test_04_datasource.vocabs)), len(test_04_result))\n",
        "print(test_04_result[0], test_04_datasource.vocabs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7gLm0MCAVik",
        "outputId": "e9143967-a72f-4eaf-88c6-069957acc9e3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1705 2922\n",
            "ダイガク 大学\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### コンバーターの初期化と比較\n",
        "*   MeCabとCoeFontの出力を比較する\n",
        "*   比較が一致しなかった場合、他のコンバータで検証する。\n",
        "*   すべてのコンバータの出力をdict型（keyに名詞、valueに各コンバーターの結果の配列）にまとめる\n",
        "\n"
      ],
      "metadata": {
        "id": "AI8rDV0dF37t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_04_converters = [TempCoeFontConverter(), GooConverter(app_id=secrets[\"goo_api_id\"]), YahooConverter(app_id=secrets[\"yahoo_app_id\"])]\n",
        "test_04_competition = ConverterCompetition(converters=test_04_converters)\n",
        "\n",
        "print(test_04_competition.compete_to_all(test_04_datasource, reset_cache=False))\n",
        "\n",
        "# print(test_04_competition.compete_to_mecab(test_04_datasource, converter_id=0, reset_cache=False)[0]) # noteの記事をデータソースにCoeFontを検証する。\n",
        "# print(test_04_competition.compete_to(test_04_datasource, subject_converter_id=1, target_converter_id=0, reset_cache=False)[0]) # noteの記事をConefont converterとgoo converterで比較する。\n",
        "# print(test_04_competition.compete_to(test_04_datasource, subject_converter_id=2, target_converter_id=0, reset_cache=False)[0]) # noteの記事をConefont converterとyahoo converterで比較する。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m4N-jM6QbBJ",
        "outputId": "51ca8e91-269b-4940-885d-94ce1b6dd25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'大学': ['ダイガク', 'ダイガク', 'ダイガク'], '月': ['ガツ', 'ツキ', 'ツキ']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 単語（データセット単位）のストリーミングとバッチ処理\n",
        "* データソースを主導で作成するのではなく、永続的に新しいデータソースから単語を取得できるエコシステムを構築する\n",
        "\n",
        "* WebhookやWebsoket, Server-sent Eventなどを用いてデータソースを自動で収集する\n",
        "(Wikipediaのstreaming機能など)\n",
        "\n",
        "## 要件\n",
        "*   永続的にプログラムがデータソースを自動で生成する\n",
        "*   単語のバリエーションの広がりを担保する\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1yAaRptcOxlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompetionBatch:\n",
        "  def __init__(self, competiton, streaming_iters, batch_character_size=200):\n",
        "    self.competiton = competiton\n",
        "    self.streaming_iters = streaming_iters\n",
        "    self.batch_character_size = batch_character_size\n",
        "\n",
        "  def start(self, reset_cache=False):\n",
        "    entries = []\n",
        "    character_size = 0\n",
        "    # 取得したデータソースをもとに比較処理\n",
        "    for datasource in self.streaming_iters:\n",
        "       # まだspreadsheetに登録されていない単語だけを抽出\n",
        "       filted_vocabs = [ (vocab, label) for vocab, label in zip(datasource.vocabs, datasource.labels) if checked_db.get(vocab.encode(\"utf-8\")) == None]\n",
        "\n",
        "       #バッチ処理のエントリーに追加+文字数を加算\n",
        "       entries += filted_vocabs\n",
        "       vocab_sizes = list(map(lambda vocab:len(vocab[0]), filted_vocabs))\n",
        "       character_size += sum(vocab_sizes)\n",
        "\n",
        "       # 新しく比較対象にエントリーされた総文字数がbatch_character_size以上になった場合、比較処理を行う\n",
        "       if self.batch_character_size <= character_size:\n",
        "         print(\"start new batch!\")\n",
        "         entry_vocabs = list(map(lambda vocab: vocab[0], entries))\n",
        "         entry_labels = list(map(lambda vocab: vocab[1], entries))\n",
        "         dataset = ArrayDataSource(entry_vocabs, entry_labels) # バッチ処理に必要なデータセットを作成\n",
        "         errors = self.competiton.compete_to_all(dataset, reset_cache) # coefont apiとmecab and 他社apiを比較する\n",
        "\n",
        "         self.save_to_spreadsheet(errors) # google spreadsheetに結果を書き込み\n",
        "         # 初期化\n",
        "         entries = []\n",
        "         character_size = 0\n",
        "\n",
        "  def save_to_spreadsheet(self, errors):\n",
        "    print(errors)"
      ],
      "metadata": {
        "id": "jPBp0vU8QRaM"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データセットの自動取得（wikipedia rechange logs）\n",
        "* データソースを主導で作成するのではなく、永続的に新しいデータソースから単語を取得できるエコシステムを構築する\n",
        "\n",
        "## 要件\n",
        "*   永続的にプログラムがデータソースを自動で生成できる\n",
        "*   単語数に基づき、バッチ処理的に単語の比較とspreadsheetへのアノテーションが行われる。\n",
        "*   単語のバリエーションの広がりを担保する\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DPhw0_3emQvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sseclient\n",
        "from sseclient import SSEClient as EventSource"
      ],
      "metadata": {
        "id": "s3E7J0VjmALH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_09_converters = [\n",
        "    GooConverter(app_id=secrets[\"goo_api_id\"]),\n",
        "    YahooConverter(app_id=secrets[\"yahoo_app_id\"])]\n",
        "\n",
        "test_09_competition = ConverterCompetition(converters=test_09_converters)\n",
        "\n",
        "# wikipediaの更新ログをデータセットに変換するイテレータ\n",
        "def getDataSourceFromWikipedia(limit=20):\n",
        "  count = 0\n",
        "  for event in EventSource(test_09_url, last_id=None):\n",
        "    if event.event == 'message':\n",
        "        try:\n",
        "            change = json.loads(event.data)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        else:\n",
        "            if change[\"server_name\"] == 'ja.wikipedia.org':\n",
        "                datasource = WikipediaDataSource(title=change[\"title\"])\n",
        "                datasource.load()\n",
        "                # print(\"recieved:\", change[\"title\"], \"new count:\", len(datasource.vocabs))\n",
        "                count +=1\n",
        "                if count < limit:\n",
        "                   yield datasource\n",
        "                else:\n",
        "                   break\n",
        "\n",
        "test_09_batch = CompetionBatch(test_09_competition, streaming_iters=getDataSourceFromWikipedia(), batch_character_size=20)\n",
        "test_09_batch.start() #ストリーミング処理を開始"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx1kN4rSYBuy",
        "outputId": "da65559f-0904-4647-d5a7-900d3aee3d38"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start new batch!\n",
            "** Goo API: {\"converted\": \"ロクオン。 シンゴウ。 ショリ。 ゴ。 オオヤケ。 キサキ。 サンカ。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437258\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"おおやけ\",\"roman\":\"ooyake\",\"surface\":\"公\"},{\"surface\":\"。\"},{\"furigana\":\"きさき\",\"roman\":\"kisaki\",\"surface\":\"妃\"},{\"surface\":\"。\"}]}}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '公': ['コウ:mecab', 'オオヤケ:c_0', 'オオヤケ'], '妃': ['ヒ:mecab', 'キサキ:c_0', 'キサキ'], '一': ['ヒト:mecab', 'イチ:c_0', '一']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"ホウリツ。 セイサク。 ホウレイ。 バンゴウ。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437269\\t0\"}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"ガイコウ。 ウマレ。 エイコク。 リョウジ。 フニン。 タイシ。 ケンム。 ベンム。 コウシ。 ヨウショク。 ハイグウ。 コウゴ。 ブンタン。 イクジ。 リョウリツ。 オット。 トモ。 ガイム。 レンポウ。 ニュウショウ。 ガイコク。 ガクシュウ。 テキセイ。 ケンサ。 セイセキ。 シュウトク。 ナンイ。 テイアン。 チュウキントウ。 チイキ。 キョウミ。 セイチョウ。 キキワケ。 ハンダン。 ショウキョ。 エン。 ハジマリ。 ロク。 カヅキ。 エイ。 コウエン。 カイガイ。 チ。 ココロ。 ヨロコビ。 タンノウ。 カツヨウ。 セッキョク。 ハッシン。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437270\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"かげつ\",\"roman\":\"kagetu\",\"surface\":\"ヶ月\"},{\"surface\":\"。\"}]}}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], 'ヶ月': ['カゲツ:mecab', 'カヅキ:c_0', 'カゲツ']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"タケモト。 ダンセイ。 ヤマグチ。 トクヤマ。 シュウナン。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437272\\t0\"}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], '二': ['ニ:mecab', 'ニー:c_0', '二']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"ゼンコク。 ケモノシン。 エンブ。 オウゴン。 シュウ。 ゲンアン。 シャ。 アヤ。 サクガ。 ヒロシ。 カクゲツ。 ドウシ。 サイゴ。 キュウカン。 ショウネン。 イセキ。 イゴ。 キュウサイ。 セイサク。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437293\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"じゅうしん\",\"roman\":\"zyuusin\",\"surface\":\"獣神\"},{\"surface\":\"。\"},{\"furigana\":\"あや\",\"roman\":\"aya\",\"surface\":\"綾\"},{\"surface\":\"。\"}]}}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '獣神': ['ジュウシン:mecab', 'ケモノシン:c_0', 'ジュウシン'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], '綾': ['リョウ:mecab', 'アヤ:c_0', 'アヤ'], '巻': ['マキ:mecab', 'カン:c_0', 'マキ']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"カクトウ。 カクトウ。 ハス。 シガ。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437296\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"はす\",\"roman\":\"hasu\",\"surface\":\"蓮\"},{\"surface\":\"。\"}]}}\n",
            "{'蓮': ['レン:mecab', 'ハス:c_0', 'ハス'], '年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], '時': ['ジ:mecab', 'トキ:c_0', 'トキ']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"シュウ。 キオク。 コダイ。 ウチュウ。 ヒコウ。 テイシュツ。 カガク。 タスウ。 キョゼツ。 ニセシ。 ニセ。 コウコ。 イッポウ。 テイショウ。 ライホウ。 シジ。 チキュウ。 セイメイ。 タンサ。 キョウドウ。 セッケイ。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437298\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"にせ\",\"roman\":\"nise\",\"surface\":\"偽\"},{\"furigana\":\"し\",\"roman\":\"si\",\"surface\":\"史\"},{\"surface\":\"。\"}]}}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '偽史': ['ギシ:mecab', 'ニセシ:c_0', 'ニセシ'], '体': ['タイ:mecab', 'カラダ:c_0', 'カラダ']}\n",
            "start new batch!\n",
            "{'倫': ['オサム:mecab', 'リン:c_0', 'ヒトシ'], '年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], '杯': ['ハイ:mecab', 'サカズキ:c_0', 'ハイ']}\n",
            "start new batch!\n",
            "** Goo API: {\"converted\": \"アイコ。 シンノウ。 コウゾク。 ショウゴウ。 トシノミヤ。 イン。 ゴヨウ。 ケイショウ。 デンカ。 ナルヒト。 テンノウ。 コウ。 ハハ。 マサコ。 コウゴウ。 アキヒト。 ジョウコウ。 ミチコ。 コウソン。 タンジョウ。 ハツ。 れいわ。 コウシツ。 ネンショウ。 ウチテイ。 ジュウキョ。 チヨダ。 コウキョ。 ゴショ。\", \"output_type\": \"katakana\", \"request_id\": \"labs.goo.ne.jp\\t1663437301\\t0\"}\n",
            "** Yahoo API: {\"id\":\"abc\",\"jsonrpc\":\"2.0\",\"result\":{\"word\":[{\"furigana\":\"しるし\",\"roman\":\"sirusi\",\"surface\":\"印\"},{\"surface\":\"。\"},{\"furigana\":\"なるひと\",\"roman\":\"naruhito\",\"surface\":\"徳仁\"},{\"surface\":\"。\"},{\"furigana\":\"のりかず\",\"roman\":\"norikazu\",\"surface\":\"令和\"},{\"surface\":\"。\"},{\"furigana\":\"ないてい\",\"roman\":\"naitei\",\"surface\":\"内廷\"},{\"surface\":\"。\"}]}}\n",
            "{'年': ['ネン:mecab', 'トシ:c_0', 'トシ'], '月': ['ガツ:mecab', 'ツキ:c_0', 'ツキ'], '日本': ['ニッポン:mecab', 'ニホン:c_0', 'ニホン'], '印': ['シルシ:mecab', 'イン:c_0', 'シルシ'], '徳仁': ['トクジン:mecab', 'ナルヒト:c_0', 'ナルヒト'], '令和': ['レイワ:mecab', 'れいわ:c_0', 'ノリカズ'], '名': ['メイ:mecab', 'ナ:c_0', 'ナ'], '内廷': ['ナイテイ:mecab', 'ウチテイ:c_0', 'ナイテイ'], '都': ['ト:mecab', 'ミヤコ:c_0', 'ト']}\n"
          ]
        }
      ]
    }
  ]
}